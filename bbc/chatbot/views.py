from distutils.log import error
from xml.etree.ElementPath import prepare_predicate
from django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

import json
import numpy as np
import random

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
import tensorflow_datasets as tfds
import os
import re
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from konlpy.tag import Okt

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜
D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›
NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ 
UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°
DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨

df = pd.read_csv('static/no_nan_qna_set.csv', index_col=0)

# okt = Okt()
# okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ í† í°í™”í•œ ë¬¸ì˜ë‚´ì—­, subtype ë¦¬ìŠ¤íŠ¸ ì €ì¥
inquiry_token = []

# for i in range(len(df)):
#   inquiry_token.append(okt.morphs(df['inquiry'][i]))

# okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ í†µí•´ ë¶„ë¦¬ëœ ê¸°ì¤€ìœ¼ë¡œ ë„ì–´ì“°ê¸° í•˜ê¸°
# ë°©ë²• 1
# for sentence in inquiry_token[:2]: # ['ë¸Œë ˆì´í¬', 'ê°€', 'ë§ì´', 'ë°€ë¦¼', 'ì°¨ëŸ‰', 'ë³€ê²½', 'ìš”ì²­']
#   temp = ''
#   for word in sentence: # ë¸Œë ˆì´í¬ ê°€ ë§ì´ ë°€ë¦¼ ì°¨ëŸ‰ ë³€ê²½ ìš”ì²­
#     temp += word + ' '
#   temp = temp.rstrip()
# ë°©ë²• 2 -> ì±„íƒ
# questions_okt = []
# for sentence in inquiry_token:
#   temp = ' '.join(sentence)
#   questions_okt.append(temp)

########### ë‹¨ì–´ì¥ ë§Œë“¤ê¸° ###########
import tensorflow_datasets as tfds
print("ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘")

questions = list(df['inquiry'])
# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±. (Tensorflow 2.3.0 ì´ìƒ) (í´ë¼ìš°ë“œëŠ” 2.4 ì…ë‹ˆë‹¤)
tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions, target_vocab_size=2**13) # questionsë§Œ!
# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]
# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.
VOCAB_SIZE = tokenizer.vocab_size + 2
tokenizer_questions = []
for i in range(len(questions)):
  tokenizer_questions.append(tokenizer.encode(questions[i]))

# # ìƒˆë¡­ê²Œ word_to_index ìƒì„±!!!!!
# # X ìë¦¬ì— tokenizer_questions ë„£ê¸°
# from collections import Counter

# # def make_word_to_index(X):
# # words = np.concatenate(tokenizer.subwords).tolist()
# counter = Counter(tokenizer.subwords) # ê° ìš”ì†Œì˜ ê°œìˆ˜ ë‹¤ë£¨ê³  ì‹¶ì„ ë•Œ
# counter = counter.most_common(2774-4) # ë¹ˆë„ìˆœìœ¼ë¡œ ë†’ì€ 9996ê°œ ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ íŠœí”Œë¡œ ë°˜í™˜
# vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter] # vocab ì €ì¥
# word_to_index = {word: index for index, word in enumerate(vocab)}
# index_to_word = {index: word for word, index in word_to_index.items()}

########### ë¶ˆìš©ì–´ ì œê±° ###########
import re
new_memo = []
for i in range(len(df)):
    sentence = re.split("[\n | / | ) | , | _ | ì¬ì¸ì… | ì¸ì… | ë¬¸ì˜ | ìš”ì²­ | í™•ì¸]", df['inquiry'][i]) # ê·¸ëƒ¥ splitì€ ì—¬ëŸ¬ê°œ ì•ˆë¼ì„œ, re.split ì‚¬ìš©
    temp = ' '.join(sentence)
    temp = re.sub(r"\s+", " ", temp).strip()
    new_memo.append(temp)
df['inquiry'] = new_memo

########### í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´ ###########
class PositionalEncoding(tf.keras.layers.Layer):

  def __init__(self, position, d_model):
    super(PositionalEncoding, self).__init__()
    self.pos_encoding = self.positional_encoding(position, d_model)
  
  def get_config(self):
      config = super().get_config()
      config.update({
          "pos_encoding": self.pos_encoding,
      })
      return config

  def get_angles(self, position, i, d_model):
    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
    return position * angles

  def positional_encoding(self, position, d_model):
    # ê°ë„ ë°°ì—´ ìƒì„±
    angle_rads = self.get_angles(
        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
        d_model=d_model)

    # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©
    sines = tf.math.sin(angle_rads[:, 0::2])
    # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©
    cosines = tf.math.cos(angle_rads[:, 1::2])

    # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´
    pos_encoding = tf.stack([sines, cosines], axis=0)
    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) 
    pos_encoding = tf.reshape(pos_encoding, [position, d_model])

    pos_encoding = pos_encoding[tf.newaxis, ...]
    return tf.cast(pos_encoding, tf.float32)

  def call(self, inputs):
    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]

########### ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜ ###########
def scaled_dot_product_attention(query, key, value, mask):
  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”
  depth = tf.cast(tf.shape(key)[-1], tf.float32)
  logits = matmul_qk / tf.math.sqrt(depth)

  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€
  if mask is not None:
    logits += (mask * -1e9)

  # softmaxì ìš©
  attention_weights = tf.nn.softmax(logits, axis=-1)

  # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸
  output = tf.matmul(attention_weights, value)
  return output

########### ë©€í‹°í—¤ë“œ ì–´í…ì…˜ ###########
# ë‚´ë¶€ì ìœ¼ë¡œëŠ” ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤
class MultiHeadAttention(tf.keras.layers.Layer):

  def __init__(self, d_model, num_heads, name="multi_head_attention"):
    super(MultiHeadAttention, self).__init__(name=name)
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.query_dense = tf.keras.layers.Dense(units=d_model)
    self.key_dense = tf.keras.layers.Dense(units=d_model)
    self.value_dense = tf.keras.layers.Dense(units=d_model)

    self.dense = tf.keras.layers.Dense(units=d_model)
  
  def get_config(self):
    config = super().get_config()
    config.update({
        "num_heads": self.num_heads,
        "d_model": self.d_model,
        "depth": self.depth,
        "query_dense": self.query_dense,
        "key_dense": self.key_dense,
        "value_dense": self.value_dense,
        "dense": self.dense,
    })
    return config

  def split_heads(self, inputs, batch_size):
    inputs = tf.reshape(
        inputs, shape=(batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(inputs, perm=[0, 2, 1, 3])

  def call(self, inputs):
    query, key, value, mask = inputs['query'], inputs['key'], inputs[
        'value'], inputs['mask']
    batch_size = tf.shape(query)[0]

    # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤
    query = self.query_dense(query)
    key = self.key_dense(key)
    value = self.value_dense(value)

    # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤
    query = self.split_heads(query, batch_size)
    key = self.split_heads(key, batch_size)
    value = self.split_heads(value, batch_size)

    # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜
    scaled_attention = scaled_dot_product_attention(query, key, value, mask)

    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

    # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤
    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))

    # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤
    outputs = self.dense(concat_attention)

    return outputs

########### íŒ¨ë”© ë§ˆìŠ¤í‚¹ í•¨ìˆ˜ ###########
def create_padding_mask(x):
  mask = tf.cast(tf.math.equal(x, 0), tf.float32)
  # (batch_size, 1, 1, sequence length)
  return mask[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(x):
  seq_len = tf.shape(x)[1]
  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
  padding_mask = create_padding_mask(x)
  return tf.maximum(look_ahead_mask, padding_mask)

########### ì¸ì½”ë” ë ˆì´ì–´ ###########
# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.
# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
def encoder_layer(units, d_model, num_heads, dropout, name="encoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")

  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)
  attention = MultiHeadAttention(
      d_model, num_heads, name="attention")({
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': padding_mask
      })

  # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  attention = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(inputs + attention)

  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ
  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention + outputs)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
    

########### ì¸ì½”ë” í•¨ìˆ˜ ###########
def encoder(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name="encoder"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")

  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©
  padding_mask = tf.keras.Input(shape=(1, 1, None), name="padding_mask")

  # ì„ë² ë”© ë ˆì´ì–´
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))

  # í¬ì§€ì…”ë„ ì¸ì½”ë”©
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)

  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.
  for i in range(num_layers):
    outputs = encoder_layer(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name="encoder_layer_{}".format(i),
    )([outputs, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)


########### ë””ì½”ë” ë ˆì´ì–´ ###########
# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.
# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.
def decoder_layer(units, d_model, num_heads, dropout, name="decoder_layer"):
  inputs = tf.keras.Input(shape=(None, d_model), name="inputs")
  enc_outputs = tf.keras.Input(shape=(None, d_model), name="encoder_outputs")
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name="look_ahead_mask")
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')

  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)
  attention1 = MultiHeadAttention(
      d_model, num_heads, name="attention_1")(inputs={
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': look_ahead_mask
      })

  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
  attention1 = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention1 + inputs)

  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)
  attention2 = MultiHeadAttention(
      d_model, num_heads, name="attention_2")(inputs={
          'query': attention1,
          'key': enc_outputs,
          'value': enc_outputs,
          'mask': padding_mask
      })

  # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”
  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)
  attention2 = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention2 + attention1)

  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ
  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)

  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(outputs + attention2)

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)


########### ë””ì½”ë” í•¨ìˆ˜ ###########
def decoder(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name='decoder'):
  inputs = tf.keras.Input(shape=(None,), name='inputs')
  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')
  look_ahead_mask = tf.keras.Input(
      shape=(1, None, None), name='look_ahead_mask')

  # íŒ¨ë”© ë§ˆìŠ¤í¬
  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')
  
  # ì„ë² ë”© ë ˆì´ì–´
  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)
  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))

  # í¬ì§€ì…”ë„ ì¸ì½”ë”©
  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)

  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰
  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)

  for i in range(num_layers):
    outputs = decoder_layer(
        units=units,
        d_model=d_model,
        num_heads=num_heads,
        dropout=dropout,
        name='decoder_layer_{}'.format(i),
    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])

  return tf.keras.Model(
      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],
      outputs=outputs,
      name=name)

# original
questions = list(df['inquiry'])
answers = list(df['sub_type'])

total_data_text = tokenizer_questions # ìœ„ì—ì„œ ë¶ˆìš©ì–´ ì œê±°í•œ questionsë¥¼ encoding í•œ ê²ƒ
# í…ìŠ¤íŠ¸ ë°ì´í„° ë¬¸ì¥ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸ ìƒì„±

num_tokens = [len(tokens) for tokens in total_data_text]
num_tokens = np.array(num_tokens)

max_tokens = np.mean(num_tokens) + 2*np.std(num_tokens)
maxlen = int(max_tokens)

# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´
MAX_LENGTH = maxlen

# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©
def tokenize_and_filter(inputs, outputs):
  tokenized_inputs, tokenized_outputs = [], []
  
  for (sentence1, sentence2) in zip(inputs, outputs):
    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€
    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN
    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN

    # ìµœëŒ€ ê¸¸ì´ 25 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©
    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:
      tokenized_inputs.append(sentence1)
      tokenized_outputs.append(sentence2)
  
  # ìµœëŒ€ ê¸¸ì´ 26ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©
  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(
      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')
  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(
      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')
  
  return tokenized_inputs, tokenized_outputs

# original questions, answer ì´ìš©
questions, answers = tokenize_and_filter(questions, answers)

BATCH_SIZE = 64
BUFFER_SIZE = 2774

# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.
dataset = tf.data.Dataset.from_tensor_slices((
    {
        'inputs': questions,
        'dec_inputs': answers[:, :-1]
    },
    {
        'outputs': answers[:, 1:]
    },
))

dataset = dataset.cache()
dataset = dataset.shuffle(BUFFER_SIZE)
dataset = dataset.batch(BATCH_SIZE)
dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

########### íŠ¸ëœìŠ¤í¬ë¨¸ ###########
def transformer(vocab_size,
                num_layers,
                units,
                d_model,
                num_heads,
                dropout,
                name="transformer"):
  inputs = tf.keras.Input(shape=(None,), name="inputs")
  dec_inputs = tf.keras.Input(shape=(None,), name="dec_inputs")

  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬
  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)

  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask,
      output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)

  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹
  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  # ì¸ì½”ë”
  enc_outputs = encoder(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[inputs, enc_padding_mask])

  # ë””ì½”ë”
  global dec_outputs
  dec_outputs = decoder(
      vocab_size=vocab_size,
      num_layers=num_layers,
      units=units,
      d_model=d_model,
      num_heads=num_heads,
      dropout=dropout,
  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  # ì™„ì „ì—°ê²°ì¸µ
  outputs = tf.keras.layers.Dense(units=vocab_size, name="outputs")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)

########### ì†ì‹¤ í•¨ìˆ˜ ###########
def loss_function(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
  
  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none')(y_true, y_pred)

  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
  loss = tf.multiply(loss, mask)

  return tf.reduce_mean(loss)

########### ì»¤ìŠ¤í…€ëœ í•™ìŠµë¥  ###########
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super(CustomSchedule, self).__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps**-1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
  
sample_learning_rate = CustomSchedule(d_model=128)

learning_rate = CustomSchedule(D_MODEL)

optimizer = tf.keras.optimizers.Adam(
    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

def accuracy(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))
  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
  return tf.keras.metrics.categorical_accuracy(y_true, y_pred)

model = transformer(
        vocab_size=VOCAB_SIZE,
        num_layers=NUM_LAYERS,
        units=UNITS,
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        dropout=DROPOUT)

# model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])

# def decoder_inference(sentence):
# #   sentence = preprocess_sentence(sentence)

#   # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.
#   # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]
#   sentence = tf.expand_dims(
#       START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)

#   # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.
#   # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331
#   output_sequence = tf.expand_dims(START_TOKEN, 0)

#   # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„
#   for i in range(MAX_LENGTH):
#     # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
#     predictions = model(inputs=[sentence, output_sequence], training=False)
#     predictions = predictions[:, -1:, :]

#     # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜m
#     predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

#     # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ
#     if tf.equal(predicted_id, END_TOKEN[0]):
#       break

#     # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.
#     # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
#     output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)

#   return tf.squeeze(output_sequence, axis=0)

# def sentence_generation(sentence):
#   # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.
#   prediction = decoder_inference(sentence)

#   # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#   predicted_sentence = tokenizer.decode(
#       [i for i in prediction if i < tokenizer.vocab_size])

#   print('ì…ë ¥ : {}'.format(sentence))
#   print('ì¶œë ¥ : {}'.format(predicted_sentence))

#   return predicted_sentence


##########################################################################################################
def home(request):
    context = {}
    return render(request, "chathome.html", context)

@csrf_exempt
def chattrain(request):
    context = {}

    print('chattrain ---> +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')

    tf.keras.backend.clear_session()

    model = transformer(
        vocab_size=VOCAB_SIZE,
        num_layers=NUM_LAYERS,
        units=UNITS,
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        dropout=DROPOUT)

    model.summary()

    context['result_msg'] = 'Success'
    return JsonResponse(context, content_type="application/json")

model = transformer(
        vocab_size=VOCAB_SIZE,
        num_layers=NUM_LAYERS,
        units=UNITS,
        d_model=D_MODEL,
        num_heads=NUM_HEADS,
        dropout=DROPOUT)
model.load_weights('static/chat_model/new8000_model.h5')

def decoder_inference(sentence):
#   sentence = preprocess_sentence(sentence)

  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.
  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]
  sentence = tf.expand_dims(
      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)

  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.
  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331
  output_sequence = tf.expand_dims(START_TOKEN, 0)

  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„
  for i in range(MAX_LENGTH):
    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
    predictions = model(inputs=[sentence, output_sequence], training=False)
    predictions = predictions[:, -1:, :]

    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜m
    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ
    if tf.equal(predicted_id, END_TOKEN[0]):
      break

    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.
    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)

  return tf.squeeze(output_sequence, axis=0)

# def navi_flow():
#   msg =  "ê³ ì¥ì´ë©´ 1ë²ˆ, ë¶„ì‹¤ì´ë©´ 2ë²ˆ"
#   return msg

def get_category(sub_ty):
    ty_dict = {}
    error_msg = "ì¶”ì¶œ ëœ ë¬¸ì œ: " + sub_ty + "<br> í˜„ì¬ ë“±ë¡ë˜ì§€ ì•Šì€ ì‹œë‚˜ë¦¬ì˜¤ ì…ë‹ˆë‹¤."

    if sub_ty == 'ë‚´ë¹„ê²Œì´ì…˜':
      ty_dict['msg'] = '"' + sub_ty + '"' + " ê´€ë ¨ ë¶ˆí¸ ì‚¬í•­ì„ ê²ªê³  ê³„ì‹œêµ°ìš”, ì£„ì†¡í•©ë‹ˆë‹¤.<br> ë‹¤ìŒ ì¤‘ ì–´ë–¤ ìƒí™© ì´ì‹ ê°€ìš”? ì•„ë˜ ë³´ê¸° ì¤‘ í´ë¦­í•´ì£¼ì„¸ìš”."
      ty_dict['bool'] = True
      return ty_dict
    else:
      ty_dict['msg'] = error_msg
      ty_dict['bool'] = False
      return ty_dict

# def for_broken(clicked_broken):
#   cb_dict = {}
#   if clicked_broken == "ê³ ì¥":
#     cb_dict['msg'] = 'ë‚´ë¹„ê²Œì´ì…˜ì´ ê³ ì¥ì´ ë‚˜ì„œ ë§¤ìš° ë¶ˆí¸í•˜ì…¨ê² ìŠµë‹ˆë‹¤.. <br> í˜¹ì‹œ ë‚´ë¹„ê²Œì´ì…˜ì„ ê»ë‹¤ ì¼œ ë³´ì…”ë„ ë™ì¼í•˜ê²Œ ì•ˆë˜ë‚˜ìš”?'
#     cb_dict['bool'] = True
#     return cb_dict
#   else:
#     cb_dict['msg'] = 'ë‚´ë¹„ê²Œì´ì…˜ì„ ë¶„ì‹¤í•˜ì…”ì„œ ë§¤ìš° ë‹¹í™©í•˜ì…¨ê² ìŠµë‹ˆë‹¤.. <br> í˜¹ì‹œ ê´œì°®ìœ¼ì‹œë‹¤ë©´ ê°œì¸ íœ´ëŒ€í°ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ì‹¤ê¹Œìš”?'
#     cb_dict['bool'] = False
#     return cb_dict

@csrf_exempt
def chatanswer(request):
    context = {}
    questext = request.GET['questext']
    # questext = request
    print("questtext:", questext)
    # def sentence_generation(questext):
    # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.

    prediction = decoder_inference(questext)

    # # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    predicted_sentence = tokenizer.decode(
        [i for i in prediction if i < tokenizer.vocab_size])

    # print('ì…ë ¥ : {}'.format(sentence))
    # print('ì¶œë ¥ : {}'.format(predicted_sentence))

    # return predicted_sentence

    # anstext = predicted_sentence
    # print(anstext)
    my_msg = get_category(predicted_sentence)['msg']
    my_bool = get_category(predicted_sentence)['bool']

    context['anstext'] = my_msg
    context['flag'] = '0'
    context['isNavy'] = my_bool

    return JsonResponse(context, content_type="application/json")
    # return anstext

# def sentence_generation(questext):
#     #ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.
#     prediction = decoder_inference(questext)

#     #ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
#     predicted_sentence = tokenizer.decode(
#         [i for i in prediction if i < tokenizer.vocab_size])

#     print('ì…ë ¥ : {}'.format(sentence))
#     print('ì¶œë ¥ : {}'.format(predicted_sentence))

#     return predicted_sentence

# print(sentence_generation('ë‚´ë¹„ê²Œì´ì…˜ì´ ì•ˆë¼ìš”'))
# print(chatanswer('ë‚´ë¹„ê²Œì´ì…˜ì´ ì•ˆë¼ìš”'))
# sub_type_mapping = {
#     'ë‚´ë¹„ê²Œì´ì…˜': 0,
#     'íƒ€ì´ì–´': 1,
#     'ë¼ì´íŠ¸': 2,
#     'ì‹œë™': 3,
#     'ê²½ê³ ë“±': 4,
#     'ì°¨ëŸ‰ì™¸ë¶€': 5,
#     'ì°¨ëŸ‰ë‚´ë¶€': 6,
#     'ì£¼í–‰ê´€ë ¨': 7,
#     'ì‚¬ê³ ì¡°ì‚¬': 8,
#     'ë‹¨ë§ê¸°': 9,
#     'ì£¼ìœ /ì¶©ì „ì¹´ë“œ': 10,
#     'í›„ë°©ì¹´ë©”ë¼': 11,
#     'í•˜ì´íŒ¨ìŠ¤': 12,
#     'ì°¨ëŸ‰ì ê²€': 13,
#     'ë¸Œë ˆì´í¬': 14,
#     'ë¸”ë™ë°•ìŠ¤': 15,
#     'ìœ„ìƒë¬¸ì œ': 16,
#     'ì£¼ì°¨ì¥': 17,
#     'ADAS': 18,
#     'ë¹„ì¹˜í’ˆ': 19,
#     'ì¶©ì „ê¸°í™•ì¸': 20
# }
